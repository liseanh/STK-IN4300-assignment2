\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{STK-IN4300 Statistical learning methods in Data Science: Assignment 2}
    \author{Anh-Nguyet Lise Nguyen}

    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}    
\maketitle
    
    


\hypertarget{problem-1}{%
\section{Problem 1}\label{problem-1}}

    \hypertarget{problem-1.1}{%
\subsection{Problem 1.1}\label{problem-1.1}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} 
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd} 
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{as} \PY{n+nn}{sklm}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{as} \PY{n+nn}{skllm}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{as} \PY{n+nn}{sklms}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{as} \PY{n+nn}{sklpre}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neighbors} \PY{k}{as} \PY{n+nn}{skln}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k}{as} \PY{n+nn}{skt}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{ensemble} \PY{k}{as} \PY{n+nn}{skle}
\PY{k+kn}{import} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{neural\PYZus{}network} \PY{k}{as} \PY{n+nn}{sknn}
\PY{k+kn}{import} \PY{n+nn}{pygam} 
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} rpy2.ipython
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Import the .csv file for the ozone data into a data frame}
\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data/ozone\PYZus{}496obs\PYZus{}25vars.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{header}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{sep}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Extract variables except SEX}
\PY{n}{variables} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}  \PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{columns} \PY{o}{!=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SEX}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Onehotting the SEX categorical variable }
\PY{n}{onehot\PYZus{}sex} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{get\PYZus{}dummies}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{SEX}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{set\PYZus{}axis}\PY{p}{(}
    \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MALE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FEMALE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}
\PY{c+c1}{\PYZsh{} Inserting MALE and FEMALE into variable data frame}
\PY{n}{variables} \PY{o}{=} \PY{n}{variables}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{onehot\PYZus{}sex}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Split into training and test set}
\PY{n}{variables\PYZus{}train\PYZus{}}\PY{p}{,} \PY{n}{variables\PYZus{}test\PYZus{}} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{variables}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{variables}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FSATEM}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Which columns to scale:}
\PY{n}{col\PYZus{}continuous} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ALTER}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AGEBGEW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FLGROSS}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FNOH24}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FLTOTMED}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FO3H24}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FTEH24}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FLGEW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}  \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FFVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Defining the scaler (subtracts mean, divides by standard deviation):}
\PY{n}{scaler} \PY{o}{=} \PY{n}{sklpre}\PY{o}{.}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{variables\PYZus{}train\PYZus{}}\PY{p}{[}\PY{n}{col\PYZus{}continuous}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Standardizing the data using the scaler defined above in the continuous variables}
\PY{n}{scaled\PYZus{}cols\PYZus{}train} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{variables\PYZus{}train\PYZus{}}\PY{p}{[}\PY{n}{col\PYZus{}continuous}\PY{p}{]}\PY{p}{)}
\PY{n}{scaled\PYZus{}cols\PYZus{}test} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{variables\PYZus{}test\PYZus{}}\PY{p}{[}\PY{n}{col\PYZus{}continuous}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} List of all variable names}
\PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{variables}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{str}\PY{p}{)}

\PY{c+c1}{\PYZsh{} List of categorical variables}
\PY{n}{col\PYZus{}categorical} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{item} \PY{o+ow}{in} \PY{n}{feature\PYZus{}names}\PY{p}{:}
    \PY{k}{if} \PY{n}{item} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{col\PYZus{}continuous}\PY{p}{:}
        \PY{n}{col\PYZus{}categorical}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{item}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Data frame of the categorical variables }
\PY{n}{variables\PYZus{}categorical\PYZus{}train} \PY{o}{=} \PY{n}{variables\PYZus{}train\PYZus{}}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{col\PYZus{}categorical}\PY{p}{]}
\PY{n}{variables\PYZus{}categorical\PYZus{}test} \PY{o}{=} \PY{n}{variables\PYZus{}test\PYZus{}}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{col\PYZus{}categorical}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Data frame of the continuous variables}
\PY{n}{variables\PYZus{}continuous\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{scaled\PYZus{}cols\PYZus{}train}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{col\PYZus{}continuous}\PY{p}{)}
\PY{n}{variables\PYZus{}continuous\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{scaled\PYZus{}cols\PYZus{}test}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{col\PYZus{}continuous}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Merging the categorical and continuous variables: }

\PY{n}{variables\PYZus{}final\PYZus{}train} \PY{o}{=} \PY{n}{variables\PYZus{}categorical\PYZus{}train}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{variables\PYZus{}continuous\PYZus{}train}\PY{p}{)} 
\PY{n}{variables\PYZus{}final\PYZus{}test} \PY{o}{=} \PY{n}{variables\PYZus{}categorical\PYZus{}test}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{variables\PYZus{}continuous\PYZus{}test}\PY{p}{)}

\PY{n}{variables\PYZus{}train\PYZus{}array} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{variables\PYZus{}categorical\PYZus{}train}\PY{p}{,} \PY{n}{variables\PYZus{}continuous\PYZus{}train}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{variables\PYZus{}final\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{variables\PYZus{}train\PYZus{}array}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{col\PYZus{}categorical}\PY{p}{)}\PY{o}{+}\PY{n+nb}{list}\PY{p}{(}\PY{n}{col\PYZus{}continuous}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{n}{variables\PYZus{}test\PYZus{}array} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{[}\PY{n}{variables\PYZus{}categorical\PYZus{}test}\PY{p}{,} \PY{n}{variables\PYZus{}continuous\PYZus{}test}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\PY{n}{variables\PYZus{}final\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{variables\PYZus{}test\PYZus{}array}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{col\PYZus{}categorical}\PY{p}{)}\PY{o}{+}\PY{n+nb}{list}\PY{p}{(}\PY{n}{col\PYZus{}continuous}\PY{p}{)}\PY{p}{)}\PY{p}{)}

    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Exporting the final preprocessed data frames as .csv }
\PY{n}{variables\PYZus{}final\PYZus{}train}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data/ozone\PYZus{}train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\PY{n}{variables\PYZus{}final\PYZus{}test}\PY{o}{.}\PY{n}{to\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data/ozone\PYZus{}test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    The categorical variables were not scaled and only SEX was encoded into
FEMALE and MALE. This was not necessary for the other categorical
variables as they were boolean.

    \hypertarget{problem-1.2}{%
\subsection{Problem 1.2}\label{problem-1.2}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Import the .csv for the training set made in Problem 1.1}
\PY{n}{df\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data/ozone\PYZus{}train.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Extract explanatory features X and outcome y}
\PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{df\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{df\PYZus{}train}\PY{o}{.}\PY{n}{columns} \PY{o}{!=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FFVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{df\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FFVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Import the .csv for the test set made in Problem 1.1}
\PY{n}{df\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{data/ozone\PYZus{}test.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Extract explanatory features X and outcome y}
\PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{df\PYZus{}test}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{df\PYZus{}test}\PY{o}{.}\PY{n}{columns} \PY{o}{!=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FFVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{df\PYZus{}test}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FFVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Need to write my own function to return a summary of coefficients, std and p\PYZhy{}values}

\PY{k}{def} \PY{n+nf}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{p}{:}
    \PY{n}{array\PYZus{}1D} \PY{o}{=} \PY{k+kc}{None}
    
    \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{shape}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
        \PY{n}{array\PYZus{}1D} \PY{o}{=} \PY{k+kc}{True}
        \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Initialize OLS and calculate coefficients}
    \PY{n}{OLS} \PY{o}{=} \PY{n}{skllm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{OLS}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)} 
    \PY{n}{coefficients} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{OLS}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{,} \PY{n}{OLS}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Calculate variance and standard deviance}
    \PY{n}{newX} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{MSE} \PY{o}{=} \PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{newX}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{newX}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    
    \PY{n}{var} \PY{o}{=} \PY{n}{MSE}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{pinv}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{newX}\PY{o}{.}\PY{n}{T}\PY{p}{,}\PY{n}{newX}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{diagonal}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{std} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{var}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Then calculate p\PYZhy{}values}
    \PY{n}{ts\PYZus{}b} \PY{o}{=} \PY{n}{coefficients}\PY{o}{/} \PY{n}{std}

    \PY{n}{p} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{ts\PYZus{}b}\PY{p}{)}\PY{p}{)}
    \PY{k}{for} \PY{n}{index}\PY{p}{,} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{ts\PYZus{}b}\PY{p}{)}\PY{p}{:}
        \PY{n}{p}\PY{p}{[}\PY{n}{index}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{scipy}\PY{o}{.}\PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{newX}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
    
    \PY{c+c1}{\PYZsh{} Make a data frame of coefficients, std and p\PYZhy{}values}
    \PY{k}{if} \PY{n}{array\PYZus{}1D}\PY{p}{:}
        \PY{n}{summary} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{coefficients}\PY{p}{,} \PY{n}{std}\PY{p}{,} \PY{n}{p}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Feature}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Standard deviance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{p\PYZhy{}values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
                              \PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{intercept}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{+} \PY{p}{[}\PY{n}{feature\PYZus{}names}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        
    \PY{k}{else}\PY{p}{:}
        \PY{n}{summary} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{coefficients}\PY{p}{,} \PY{n}{std}\PY{p}{,} \PY{n}{p}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Coefficients}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Standard deviance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{p\PYZhy{}values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
                          \PY{n}{index}\PY{o}{=}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{intercept}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{+} \PY{n+nb}{list}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                           
    \PY{k}{return} \PY{n}{summary}
        
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{summary\PYZus{}train} \PY{o}{=} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{)}

\PY{n}{summary\PYZus{}test} \PY{o}{=} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Summary for the OLS training}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{summary\PYZus{}train}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Summary for the OLS training
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
           Coefficients  Standard deviance      p-values
intercept      0.000000           0.063838  1.000000e+00
ADHEU         -0.251528           0.169382  1.388256e-01
HOCHOZON      -0.124647           0.108682  2.525339e-01
AMATOP         0.070164           0.093371  4.530934e-01
AVATOP        -0.006713           0.104893  9.490247e-01
ADEKZ          0.029896           0.098487  7.617231e-01
ARAUCH        -0.051224           0.085788  5.509842e-01
FSNIGHT        0.211984           0.164447  1.985788e-01
FMILB         -0.058481           0.143574  6.841216e-01
FTIER         -0.032023           0.137196  8.156339e-01
FPOLL          0.143026           0.182896  4.349590e-01
FSPT          -0.195312           0.192001  3.100322e-01
FSATEM         0.045070           0.262663  8.639003e-01
FSAUGE         0.076757           0.143035  5.920058e-01
FSPFEI         0.098357           0.265873  7.117447e-01
FSHLAUF       -0.133204           0.193830  4.925898e-01
MALE           0.381781           0.054090  1.696887e-11
FEMALE        -0.123152           0.049102  1.278155e-02
ALTER          0.052618           0.047401  2.680495e-01
AGEBGEW        0.056261           0.040453  1.655480e-01
FLGROSS        0.442922           0.064475  5.199130e-11
FNOH24        -0.146224           0.053815  7.050426e-03
FLTOTMED      -0.036194           0.042589  3.962318e-01
FO3H24         0.107849           0.086113  2.116042e-01
FTEH24        -0.081461           0.079687  3.076570e-01
FLGEW          0.283357           0.058876  2.593528e-06
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{problem-1.3}{%
\subsection{Problem 1.3}\label{problem-1.3}}

I'm choosing to use criterias for p-value in backward elimination and
forward selection.

    \hypertarget{backward-elimination}{%
\subsubsection{Backward elimination}\label{backward-elimination}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Need to write my own backward elimination function }

\PY{k}{def} \PY{n+nf}{backward\PYZus{}elimination}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{alpha}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Make copies of X and y}
    \PY{n}{X\PYZus{}copy} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{n}{y\PYZus{}copy} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    
    \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{X\PYZus{}copy}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
    \PY{c+c1}{\PYZsh{} Calculating coefficients, standard deviations and p\PYZhy{}values as summary}
    \PY{c+c1}{\PYZsh{} of initial X and y }
    \PY{n}{summary} \PY{o}{=} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}copy}\PY{p}{,} \PY{n}{y\PYZus{}copy}\PY{p}{,} \PY{n}{feature\PYZus{}names}\PY{p}{)}
    \PY{n}{p\PYZus{}values} \PY{o}{=} \PY{n}{summary}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{p\PYZhy{}values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Find maximum p\PYZhy{}value}
    \PY{n}{p\PYZus{}max} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{p\PYZus{}values}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Iterate until p \PYZlt{}= alpha}
    \PY{k}{while} \PY{n}{p\PYZus{}max} \PY{o}{\PYZgt{}} \PY{n}{alpha}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Calculating coefficients, standard deviations and p\PYZhy{}values using summary\PYZus{}OLS function}
        \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{X\PYZus{}copy}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
        \PY{n}{summary} \PY{o}{=} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}copy}\PY{p}{,} \PY{n}{y\PYZus{}copy}\PY{p}{,} \PY{n}{feature\PYZus{}names}\PY{p}{)}
        \PY{n}{summary}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{intercept}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{p\PYZus{}values} \PY{o}{=} \PY{n}{summary}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{p\PYZhy{}values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Locating index with maximum p\PYZhy{}value and }
        \PY{c+c1}{\PYZsh{} dropping corresponding feature}
        \PY{n}{p\PYZus{}max\PYZus{}index} \PY{o}{=} \PY{n}{p\PYZus{}values}\PY{o}{.}\PY{n}{idxmax}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{X\PYZus{}copy}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{p\PYZus{}max\PYZus{}index}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Update maximum p\PYZhy{}value}
        \PY{n}{p\PYZus{}max} \PY{o}{=} \PY{n}{p\PYZus{}values}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{p\PYZus{}max\PYZus{}index}\PY{p}{]}        
    \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{X\PYZus{}copy}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}
    
    \PY{k}{return} \PY{n}{X\PYZus{}copy}\PY{p}{,} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}copy}\PY{p}{,} \PY{n}{y\PYZus{}copy}\PY{p}{,} \PY{n}{feature\PYZus{}names}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}backelim\PYZus{}1\PYZus{}train}\PY{p}{,} \PY{n}{summary\PYZus{}backelim\PYZus{}1\PYZus{}train} \PY{o}{=} \PY{n}{backward\PYZus{}elimination}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}1}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Backward elimination using threshold 0.1 }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{summary\PYZus{}backelim\PYZus{}1\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Backward elimination using threshold 0.1

            Coefficients  Standard deviance      p-values
intercept      0.000000           0.025060  1.000000e+00
MALE           0.270255           0.040935  2.462022e-10
FEMALE        -0.233679           0.039186  8.498613e-09
FLGROSS        0.511359           0.055964  0.000000e+00
FLGEW          0.268868           0.055824  2.551946e-06
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}backelim\PYZus{}2\PYZus{}train}\PY{p}{,} \PY{n}{summary\PYZus{}backelim\PYZus{}2\PYZus{}train} \PY{o}{=} \PY{n}{backward\PYZus{}elimination}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}
\PY{n}{summary\PYZus{}backelim\PYZus{}2\PYZus{}train}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Backward elimination using threshold 0.001 }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{summary\PYZus{}backelim\PYZus{}2\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Backward elimination using threshold 0.001

            Coefficients  Standard deviance      p-values
intercept      0.000000           0.026175  1.000000e+00
MALE           0.277038           0.042734  4.854697e-10
FEMALE        -0.239544           0.040907  1.506194e-08
FLGROSS        0.709517           0.039626  0.000000e+00
    \end{Verbatim}

    \hypertarget{forward-selection}{%
\subsubsection{Forward selection}\label{forward-selection}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Need to write my own forward selection function}

\PY{k}{def} \PY{n+nf}{forward\PYZus{}selection}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{alpha}\PY{p}{)}\PY{p}{:}
    \PY{n}{X\PYZus{}copy} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{n}{y\PYZus{}copy} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{n}{p\PYZus{}max} \PY{o}{=} \PY{n}{alpha} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
    \PY{n}{p\PYZus{}min} \PY{o}{=} \PY{l+m+mi}{0}
    
    \PY{n}{feature\PYZus{}include} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{X\PYZus{}copy}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{)}
    \PY{k}{while} \PY{n}{p\PYZus{}max} \PY{o}{\PYZlt{}} \PY{n}{alpha} \PY{o+ow}{and} \PY{n+nb}{len}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{:}

        \PY{n}{n\PYZus{}features} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{)}
        \PY{n}{p\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{n\PYZus{}features}\PY{p}{)}
        
        \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{feature} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Calculate summary (coefficients, std, p\PYZhy{}value) for each feature}
            \PY{n}{features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature\PYZus{}include}\PY{p}{,} \PY{n}{feature\PYZus{}names}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
            
            \PY{n}{X\PYZus{}feature} \PY{o}{=} \PY{n}{X\PYZus{}copy}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}\PY{o}{.}\PY{n}{values}
            \PY{n}{summary} \PY{o}{=} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}feature}\PY{p}{,} \PY{n}{y\PYZus{}copy}\PY{p}{,} \PY{n}{features}\PY{p}{)}

            \PY{n}{summary}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{intercept}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{+} \PY{n}{feature\PYZus{}include} \PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
            
            
            \PY{n}{p\PYZus{}one} \PY{o}{=} \PY{n}{summary}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{p\PYZhy{}values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}
            \PY{n}{p\PYZus{}values}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{n}{p\PYZus{}one}
            
        \PY{c+c1}{\PYZsh{} Find the smallest p\PYZhy{}value and corresponding feature}
        \PY{n}{p\PYZus{}min\PYZus{}index} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{p\PYZus{}values}\PY{p}{)}
        \PY{n}{p\PYZus{}min} \PY{o}{=} \PY{n}{p\PYZus{}values}\PY{p}{[}\PY{n}{p\PYZus{}min\PYZus{}index}\PY{p}{]}
        
        \PY{n}{feature\PYZus{}min\PYZus{}p} \PY{o}{=} \PY{n}{feature\PYZus{}names}\PY{p}{[}\PY{n}{p\PYZus{}min\PYZus{}index}\PY{p}{]}
        \PY{n}{feature\PYZus{}include}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature\PYZus{}min\PYZus{}p}\PY{p}{)}
        \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{feature\PYZus{}names}\PY{p}{[}\PY{n}{feature\PYZus{}names} \PY{o}{!=} \PY{n}{feature\PYZus{}min\PYZus{}p}\PY{p}{]}

        \PY{c+c1}{\PYZsh{} Calculate summary for the included features}
        \PY{n}{X\PYZus{}feature} \PY{o}{=} \PY{n}{X\PYZus{}copy}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features}\PY{p}{]}\PY{o}{.}\PY{n}{values}

        \PY{n}{summary\PYZus{}} \PY{o}{=} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}feature}\PY{p}{,} \PY{n}{y\PYZus{}copy}\PY{p}{,} \PY{n}{feature\PYZus{}include}\PY{p}{)}
        
        \PY{n}{p\PYZus{}max} \PY{o}{=} \PY{n}{summary}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{p\PYZhy{}values}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}

        \PY{n}{X\PYZus{}final} \PY{o}{=} \PY{n}{X\PYZus{}copy}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{feature\PYZus{}include}\PY{p}{]}
        
    \PY{k}{return} \PY{n}{X\PYZus{}final}\PY{p}{,} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}final}\PY{p}{,} \PY{n}{y\PYZus{}copy}\PY{p}{,} \PY{n}{feature\PYZus{}include}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}forsec\PYZus{}1\PYZus{}train}\PY{p}{,} \PY{n}{summary\PYZus{}forsec\PYZus{}1\PYZus{}train} \PY{o}{=} \PY{n}{forward\PYZus{}selection}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}1}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Forward selection using threshold 0.1}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{summary\PYZus{}forsec\PYZus{}1\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Forward selection using threshold 0.1

            Coefficients  Standard deviance  p-values
intercept      0.000000           0.053518  1.000000
FLGROSS        0.525054           0.058168  0.000000
FLGEW          0.274859           0.058023  0.000004
MALE           0.267114           0.079119  0.000854
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}forsec\PYZus{}2\PYZus{}train}\PY{p}{,} \PY{n}{summary\PYZus{}forsec\PYZus{}2\PYZus{}train} \PY{o}{=} \PY{n}{forward\PYZus{}selection}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Forward selection using threshold 0.001}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{ }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{summary\PYZus{}forsec\PYZus{}2\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Forward selection using threshold 0.001

            Coefficients  Standard deviance  p-values
intercept      0.000000           0.053518  1.000000
FLGROSS        0.525054           0.058168  0.000000
FLGEW          0.274859           0.058023  0.000004
MALE           0.267114           0.079119  0.000854
    \end{Verbatim}

    It is interesting to note that using the same criteria \(\alpha\) for
p-value in both backward elimination and forward selection, for
\(\alpha=0.1\) the number of variables in the model is higher for
backward elimination (6) than forward selection (3). For
\(\alpha=0.001\) the backward selection model contains three variables,
while the forward selection contains two variables. The variable FLGROSS
is included in every model and seems to be the most significant. Looking
at the low p-values of the remaining variables of the new models, I
think it is likely that these models will perform better than the full
models, as eliminating the other variables with higher p-values is like
eliminating noise.

    \hypertarget{problem-1.4}{%
\subsection{Problem 1.4}\label{problem-1.4}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Need to write my own bootstrap algorithm }

\PY{k}{def} \PY{n+nf}{bootstrap}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{:}
    \PY{n}{N} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}
    \PY{n}{N\PYZus{}train} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{l+m+mf}{0.8}\PY{o}{*}\PY{n}{N}\PY{p}{)}
    \PY{n}{N\PYZus{}test} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{o}{*}\PY{n}{N}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Randomly choose indices from a list of all possible indices}
    \PY{n}{all\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{N}\PY{p}{)}    
    \PY{n}{indices\PYZus{}bootstrapped} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{indices\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{all\PYZus{}indices}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{N\PYZus{}train}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{indices\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{all\PYZus{}indices}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{N\PYZus{}test}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        
        \PY{n}{indices\PYZus{}bootstrapped}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{p}{[}\PY{n}{indices\PYZus{}train}\PY{p}{,} \PY{n}{indices\PYZus{}test}\PY{p}{]}\PY{p}{)}
        
    \PY{k}{return} \PY{n}{indices\PYZus{}bootstrapped}
    
    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{shrinkage\PYZus{}param} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{3}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1000}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Lasso with 5\PYZhy{}fold cross validation }
\PY{n}{reg\PYZus{}lasso\PYZus{}cv} \PY{o}{=} \PY{n}{skllm}\PY{o}{.}\PY{n}{LassoCV}\PY{p}{(}\PY{n}{alphas}\PY{o}{=}\PY{n}{shrinkage\PYZus{}param}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\PY{c+c1}{\PYZsh{} calculate mean square error for the test on each fold, }
\PY{c+c1}{\PYZsh{} then take the mean over all folds axis=1}
\PY{n}{MSE\PYZus{}lasso\PYZus{}cv} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{reg\PYZus{}lasso\PYZus{}cv}\PY{o}{.}\PY{n}{mse\PYZus{}path\PYZus{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Initiate bootstrap splitting}
\PY{n}{a} \PY{o}{=} \PY{n}{bootstrap}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Then use LassoCV with the bootstrap splitting instead of CV}
\PY{n}{reg\PYZus{}lasso\PYZus{}bootstrap} \PY{o}{=} \PY{n}{skllm}\PY{o}{.}\PY{n}{LassoCV}\PY{p}{(}\PY{n}{alphas}\PY{o}{=}\PY{n}{shrinkage\PYZus{}param}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{a}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\PY{n}{MSE\PYZus{}lasso\PYZus{}bootstrap} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{reg\PYZus{}lasso\PYZus{}bootstrap}\PY{o}{.}\PY{n}{mse\PYZus{}path\PYZus{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plotting expected error over hyperparameter values}
\PY{n}{figure\PYZus{}lasso} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{semilogx}\PY{p}{(}\PY{n}{shrinkage\PYZus{}param}\PY{p}{,} \PY{n}{MSE\PYZus{}lasso\PYZus{}cv}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{5\PYZhy{}fold CV}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{semilogx}\PY{p}{(}\PY{n}{shrinkage\PYZus{}param}\PY{p}{,} \PY{n}{MSE\PYZus{}lasso\PYZus{}bootstrap}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bootstrap}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Hyperparameter \PYZdl{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{lambda\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{EPE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The best shrinkage parameter for Lasso using CV is }\PY{l+s+si}{\PYZob{}reg\PYZus{}lasso\PYZus{}cv.alpha\PYZus{}:.2e\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} 
      \PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{with R2 score of }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{reg\PYZus{}lasso\PYZus{}cv.score(X\PYZus{}test,y\PYZus{}test):.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{The best shrinkage parameter for Lasso using bootstrap is }\PY{l+s+si}{\PYZob{}reg\PYZus{}lasso\PYZus{}bootstrap.alpha\PYZus{}:.2e\PYZcb{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+} 
      \PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{with R2 score of }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{reg\PYZus{}lasso\PYZus{}bootstrap.score(X\PYZus{}test,y\PYZus{}test):.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
The best shrinkage parameter for Lasso using CV is 2.32e-02 with R2 score of
0.59
The best shrinkage parameter for Lasso using bootstrap is 1.49e-02 with R2 score
of 0.59
    \end{Verbatim}

    From the plot, the EPE found using bootstrap is noticably lower than the
EPE found using 5-fold cross validation. This is due to the severe
underestimation of the EPE by the bootstrapping method, as the training
and test sets are not independent. This can be improved by using
e.g.~0.632 bootstrap.

    \hypertarget{problem-1.5}{%
\subsection{Problem 1.5}\label{problem-1.5}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Checking for linear dependence}

\PY{c+c1}{\PYZsh{} Fitting linear GAM }
\PY{n}{gam\PYZus{}check} \PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{LinearGAM}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Plotting}
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{,} \PY{n}{sharey}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{ax} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{axes}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{XX} \PY{o}{=} \PY{n}{gam\PYZus{}check}\PY{o}{.}\PY{n}{generate\PYZus{}X\PYZus{}grid}\PY{p}{(}\PY{n}{i}\PY{p}{)}
    \PY{n}{pdep}\PY{p}{,} \PY{n}{confi} \PY{o}{=} \PY{n}{gam\PYZus{}check}\PY{o}{.}\PY{n}{partial\PYZus{}dependence}\PY{p}{(}\PY{n}{term}\PY{o}{=}\PY{n}{i}\PY{p}{,} \PY{n}{X}\PY{o}{=}\PY{n}{XX}\PY{p}{,} \PY{n}{width}\PY{o}{=}\PY{l+m+mf}{0.95}\PY{p}{,} \PY{n}{meshgrid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{XX}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{pdep}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{black}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{XX}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{confi}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{green}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ls}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{XX}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{confi}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{green}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ls}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
\PY{n}{fig}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Judging from the above plots, the variables that seem nonlinear to me
are FLGROSS, FTEH24 and FLGEW.

First we try to fit a GAM:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{nonlinear\PYZus{}features} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FLGROSS}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FTEH24}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FLGEW}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{} In pygam, pygam.l indicates linear, pygam.s is splines}

\PY{c+c1}{\PYZsh{} Initializing pygam for the first variable ADHEU, which is linear:}
\PY{n}{gam\PYZus{}feature\PYZus{}type} \PY{o}{=}  \PY{n}{pygam}\PY{o}{.}\PY{n}{l}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Then for the rest of the features:}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{feature} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Splines for non\PYZhy{}linear features}
    \PY{k}{if} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{nonlinear\PYZus{}features}\PY{p}{:}
        \PY{n}{gam\PYZus{}feature\PYZus{}type} \PY{o}{+}\PY{o}{=}  \PY{n}{pygam}\PY{o}{.}\PY{n}{s}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{spline\PYZus{}order}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{n\PYZus{}splines}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}  And linear for the others}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{gam\PYZus{}feature\PYZus{}type} \PY{o}{+}\PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{l}\PY{p}{(}\PY{n}{i}\PY{p}{)}

        
\PY{c+c1}{\PYZsh{} Grid search over penalties in log space and train the model}
\PY{n}{penalties} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)}        
\PY{n}{gam} \PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{LinearGAM}\PY{p}{(}\PY{n}{gam\PYZus{}feature\PYZus{}type}\PY{p}{)}\PY{o}{.}\PY{n}{gridsearch}\PY{p}{(}
    \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{lam}\PY{o}{=}\PY{n}{penalties}
\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
100\% (150 of 150) |\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#| Elapsed Time: 0:00:15 Time:  0:00:15
    \end{Verbatim}

    There apparently is a
\href{https://github.com/dswah/pyGAM/blob/master/pygam/pygam.py\#L1672}{bug}
in the pygam library where the p-values are incorrectly calculated and
underestimated, so I have chosen not to include those results as the
p-values would be lower than they should and would not be meaningful to
interpret.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{r2\PYZus{}test\PYZus{}gam} \PY{o}{=} \PY{n}{gam}\PY{o}{.}\PY{n}{\PYZus{}estimate\PYZus{}r2}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{explained\PYZus{}deviance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{r2\PYZus{}train\PYZus{}gam} \PY{o}{=} \PY{n}{gam}\PY{o}{.}\PY{n}{\PYZus{}estimate\PYZus{}r2}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{explained\PYZus{}deviance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R\PYZca{}2 score test: }\PY{l+s+si}{\PYZob{}r2\PYZus{}test\PYZus{}gam:.2f\PYZcb{}}\PY{l+s+s2}{, R\PYZca{}2 score train: }\PY{l+s+si}{\PYZob{}r2\PYZus{}train\PYZus{}gam:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
R\^{}2 score test: 0.57, R\^{}2 score train: 0.64
    \end{Verbatim}

    Now we will try to add non-linear terms to the linear model:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} First need to undo the scaling and centering of the data for training and test set:}
\PY{n}{variables\PYZus{}train\PYZus{}new} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}
    \PY{n}{variables\PYZus{}final\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{col\PYZus{}continuous}\PY{p}{]}
\PY{p}{)}

\PY{n}{variables\PYZus{}test\PYZus{}new} \PY{o}{=} \PY{n}{scaler}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}
    \PY{n}{variables\PYZus{}final\PYZus{}test}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{col\PYZus{}continuous}\PY{p}{]}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} Turn into dataframes }
\PY{n}{variables\PYZus{}train\PYZus{}new\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{variables\PYZus{}train\PYZus{}new}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n+nb}{list}\PY{p}{(}\PY{n}{col\PYZus{}continuous}\PY{p}{)}\PY{p}{)}
\PY{n}{variables\PYZus{}test\PYZus{}new\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{variables\PYZus{}test\PYZus{}new}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n+nb}{list}\PY{p}{(}\PY{n}{col\PYZus{}continuous}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}train\PYZus{}new} \PY{o}{=} \PY{n}{variables\PYZus{}train\PYZus{}new\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{variables\PYZus{}train\PYZus{}new\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{o}{!=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FFVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{X\PYZus{}test\PYZus{}new} \PY{o}{=} \PY{n}{variables\PYZus{}test\PYZus{}new\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{variables\PYZus{}test\PYZus{}new\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{o}{!=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FFVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

\PY{n}{features\PYZus{}continuous} \PY{o}{=} \PY{n}{col\PYZus{}continuous}\PY{p}{[}\PY{n}{col\PYZus{}continuous}\PY{o}{!=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FFVC}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

\PY{n}{X\PYZus{}nonlinear\PYZus{}train} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{X\PYZus{}nonlinear\PYZus{}test} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{nonlinear\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Looping to add 2nd and 3rd degree order features of the ones I identified }
\PY{c+c1}{\PYZsh{} earlier as non\PYZhy{}linear and adding to a list}
\PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{feature} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{features\PYZus{}continuous}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{feature} \PY{o+ow}{in} \PY{n}{nonlinear\PYZus{}features}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Adding 2nd degree to the features }
        \PY{n}{X\PYZus{}nonlinear\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}new}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{X\PYZus{}nonlinear\PYZus{}test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}new}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{nonlinear\PYZus{}names}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZca{}2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Adding 3rd degree to the features }
        \PY{n}{X\PYZus{}nonlinear\PYZus{}train}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}new}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{X\PYZus{}nonlinear\PYZus{}test}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}new}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{nonlinear\PYZus{}names}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{feature} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZca{}3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Convert list into arrays, transpose arrays to get them in the right dimensions        }
\PY{n}{X\PYZus{}nonlinear\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}train}\PY{p}{)}\PY{o}{.}\PY{n}{T}
\PY{n}{X\PYZus{}nonlinear\PYZus{}test} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{T}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Now we need to scale and center the new non\PYZhy{}linear terms:}
\PY{n}{scaler\PYZus{}nonlinear} \PY{o}{=} \PY{n}{sklpre}\PY{o}{.}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}train}\PY{p}{)}

\PY{n}{X\PYZus{}nonlinear\PYZus{}train\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler\PYZus{}nonlinear}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}nonlinear\PYZus{}test\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler\PYZus{}nonlinear}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}test}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Turn into data frames of non\PYZhy{}linear features}
\PY{n}{X\PYZus{}nonlinear\PYZus{}train\PYZus{}scaled\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}train\PYZus{}scaled}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{nonlinear\PYZus{}names}\PY{p}{)}
\PY{n}{X\PYZus{}nonlinear\PYZus{}test\PYZus{}scaled\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}test\PYZus{}scaled}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{n}{nonlinear\PYZus{}names}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Join the new non\PYZhy{}linear features with the old linear ones }
\PY{n}{X\PYZus{}train\PYZus{}nonlinear} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}train\PYZus{}scaled\PYZus{}df}\PY{p}{)}

\PY{n}{X\PYZus{}test\PYZus{}nonlinear} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{X\PYZus{}nonlinear\PYZus{}test\PYZus{}scaled\PYZus{}df}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Calculate coefficients, std and p\PYZhy{}values}
\PY{n}{summary\PYZus{}nonlinear} \PY{o}{=} \PY{n}{summary\PYZus{}OLS}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nonlinear}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}nonlinear}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{)}

\PY{n}{OLS\PYZus{}nonlinear} \PY{o}{=} \PY{n}{skllm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{n}{fit\PYZus{}intercept}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nonlinear}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{R\PYZca{}2 score for nonlinear model: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{OLS\PYZus{}nonlinear.score(X\PYZus{}test\PYZus{}nonlinear, y\PYZus{}test)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n}{summary\PYZus{}nonlinear}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
R\^{}2 score for nonlinear model: 0.5874063724704847
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
           Coefficients  Standard deviance      p-values
intercept      0.000000           0.065415  1.000000e+00
ADHEU         -0.179655           0.168157  2.863953e-01
HOCHOZON      -0.054679           0.111131  6.231376e-01
AMATOP         0.063849           0.092787  4.920193e-01
AVATOP         0.055338           0.104488  5.968597e-01
ADEKZ          0.053905           0.097556  5.810695e-01
ARAUCH        -0.010327           0.085392  9.038379e-01
FSNIGHT        0.201401           0.161667  2.140253e-01
FMILB          0.006420           0.143176  9.642692e-01
FTIER         -0.049198           0.135242  7.163383e-01
FPOLL          0.132661           0.180397  4.628039e-01
FSPT          -0.195817           0.190351  3.046199e-01
FSATEM         0.095614           0.259601  7.129565e-01
FSAUGE         0.025634           0.142304  8.571932e-01
FSPFEI         0.026740           0.264044  9.194162e-01
FSHLAUF       -0.157032           0.190953  4.116643e-01
MALE           0.312190           0.053934  2.147593e-08
FEMALE        -0.203990           0.049617  5.359829e-05
ALTER          0.049344           0.047181  2.966468e-01
AGEBGEW        0.048558           0.040577  2.325798e-01
FLGROSS        2.323848          29.897032  9.381071e-01
FNOH24        -0.145135           0.053663  7.315055e-03
FLTOTMED      -0.029627           0.042048  4.817272e-01
FO3H24         0.102470           0.090602  2.591540e-01
FTEH24         1.641090           1.163667  1.597168e-01
FLGEW          4.722430           1.403373  8.871574e-04
FLGROSS\^{}2     -6.883856          60.101755  9.089054e-01
FLGROSS\^{}3      4.970747          30.253974  8.696289e-01
FTEH24\^{}2      -3.976683           2.488332  1.112921e-01
FTEH24\^{}3       2.302274           1.349919  8.936009e-02
FLGEW\^{}2       -7.930333           2.605203  2.586696e-03
FLGEW\^{}3        3.584476           1.261148  4.854215e-03
\end{Verbatim}
\end{tcolorbox}
        
    Looking at the \(R^2\) score for the non-linear model, it does not seem
to perform better than our previous models either, but seems to perform
on par with the others. What is interesting is that the p-value for
FLGEW\^{}2 and FLGEW\^{}3 seem to be rather low, so perhaps using these
non-linear features with a penalized regression method such as the Lasso
or the forward selection and backward elimination methods would yield
better results.

    \hypertarget{problem-1.6}{%
\subsection{Problem 1.6}\label{problem-1.6}}

Apparently, Python doesn't have any well-known libraries for boosting,
so this was done using R with Jupyter Notebook. Using \%\%R at the top
of the cell transforms the entire cell into an R cell.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{R} \PYZhy{}i df\PYZus{}train \PYZhy{}i df\PYZus{}test

library(compboost)
library(ggplot2)
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{R}

linear\PYZus{}boost \PYZlt{}\PYZhy{} boostLinear(data = df\PYZus{}train, target = \PYZdq{}FFVC\PYZdq{}, loss = LossQuadratic\PYZdl{}new(), trace=10)

linear\PYZus{}boost\PYZdl{}getEstimatedCoef()
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
  1/100: risk = 0.47
 10/100: risk = 0.32
 20/100: risk = 0.25
 30/100: risk = 0.22
 40/100: risk = 0.2
 50/100: risk = 0.19
 60/100: risk = 0.18
 70/100: risk = 0.17
 80/100: risk = 0.17
 90/100: risk = 0.17
100/100: risk = 0.17


Train 100 iterations in 0 Seconds.
Final risk based on the train set: 0.17

\$AGEBGEW\_linear
              [,1]
[1,] -1.203536e-18
[2,]  8.236995e-03

\$ALTER\_linear
              [,1]
[1,] -1.472307e-17
[2,]  1.930911e-02

\$FEMALE\_linear
           [,1]
[1,]  0.1892782
[2,] -0.3529398

\$FLGEW\_linear
             [,1]
[1,] 2.085534e-17
[2,] 2.511648e-01

\$FLGROSS\_linear
              [,1]
[1,] -1.170800e-16
[2,]  4.631095e-01

\$FNOH24\_linear
              [,1]
[1,]  1.412964e-17
[2,] -5.681454e-02

\$MALE\_linear
            [,1]
[1,] -0.02164369
[2,]  0.04667508

\$offset
[1] -1.360919e-15

    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{R}
spline\PYZus{}boost \PYZlt{}\PYZhy{} boostSplines(data = df\PYZus{}train, target = \PYZdq{}FFVC\PYZdq{}, loss = LossQuadratic\PYZdl{}new(), trace=10)

table(spline\PYZus{}boost\PYZdl{}getSelectedBaselearner())
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
  1/100: risk = 0.47
 10/100: risk = 0.31
 20/100: risk = 0.24
 30/100: risk = 0.2
 40/100: risk = 0.18
 50/100: risk = 0.17
 60/100: risk = 0.16
 70/100: risk = 0.15
 80/100: risk = 0.14
 90/100: risk = 0.14
100/100: risk = 0.14


Train 100 iterations in 0 Seconds.
Final risk based on the train set: 0.14


AGEBGEW\_spline   ALTER\_spline  FEMALE\_spline   FLGEW\_spline FLGROSS\_spline
            10             12             10             17             24
 FNOH24\_spline  FTEH24\_spline    MALE\_spline
             6             16              5
    \end{Verbatim}

    For component-wise boosting with trees, I wasn't able to find a similar
method in the compboost package for R. Looking online, I did manage to
find
\href{https://cran.r-project.org/web/packages/mboost/mboost.pdf}{mboost}
which says it uses ``component-wise (penalized) least squares estimates
or regression trees as base-learners'', so I hope it is correct to use
this package for this purpose. I was not able to download this package,
however. I tried troubleshooting this, but was not able to make it work.
Sorry!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{R} 
library(mboost)

tree\PYZus{}boost \PYZlt{}\PYZhy{} blackboost(data=df\PYZus{}train, target=\PYZdq{}FFVC\PYZdq{})
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
R[write to console]: Error in library(mboost) : there is no package called
‘mboost’
Calls: <Anonymous> -> <Anonymous> -> withVisible -> library

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

Error in library(mboost) : there is no package called ‘mboost’
Calls: <Anonymous> -> <Anonymous> -> withVisible -> library
    \end{Verbatim}

    \hypertarget{problem-1.7}{%
\subsection{Problem 1.7}\label{problem-1.7}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} MSE for full OLS model }
\PY{n}{OLS} \PY{o}{=} \PY{n}{skllm}\PY{o}{.}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}


\PY{n}{OLS\PYZus{}full} \PY{o}{=} \PY{n}{OLS}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}OLS\PYZus{}train} \PY{o}{=} \PY{n}{OLS\PYZus{}full}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}OLS\PYZus{}test} \PY{o}{=} \PY{n}{OLS\PYZus{}full}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}test}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for full OLS model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}test:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for full OLS model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}train:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} MSE for backward elimination model}
\PY{n}{OLS\PYZus{}backelim\PYZus{}1} \PY{o}{=} \PY{n}{OLS}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}backelim\PYZus{}1\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}OLS\PYZus{}backelim\PYZus{}1\PYZus{}train} \PY{o}{=} \PY{n}{OLS\PYZus{}backelim\PYZus{}1}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}backelim\PYZus{}1\PYZus{}train}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}backelim\PYZus{}1\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}backelim\PYZus{}1\PYZus{}train}\PY{p}{)}

\PY{n}{features\PYZus{}backelim\PYZus{}1} \PY{o}{=} \PY{n}{X\PYZus{}backelim\PYZus{}1\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}

\PY{n}{X\PYZus{}backelim\PYZus{}1\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features\PYZus{}backelim\PYZus{}1}\PY{p}{]}
\PY{n}{y\PYZus{}OLS\PYZus{}backelim\PYZus{}1\PYZus{}test} \PY{o}{=} \PY{n}{OLS\PYZus{}backelim\PYZus{}1}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}backelim\PYZus{}1\PYZus{}test}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}backelim\PYZus{}1\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}backelim\PYZus{}1\PYZus{}test}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for backwards elimination model using threshold = 0.1: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}backelim\PYZus{}1\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for backwards elimination model using threshold = 0.1: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}backelim\PYZus{}1\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} MSE for backward elimination model. }
\PY{n}{OLS\PYZus{}backelim\PYZus{}2} \PY{o}{=} \PY{n}{OLS}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}backelim\PYZus{}2\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}OLS\PYZus{}backelim\PYZus{}2\PYZus{}train} \PY{o}{=} \PY{n}{OLS\PYZus{}backelim\PYZus{}2}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}backelim\PYZus{}2\PYZus{}train}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}backelim\PYZus{}2\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}backelim\PYZus{}2\PYZus{}train}\PY{p}{)}

\PY{n}{features\PYZus{}backelim\PYZus{}2} \PY{o}{=} \PY{n}{X\PYZus{}backelim\PYZus{}2\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}

\PY{n}{X\PYZus{}backelim\PYZus{}2\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features\PYZus{}backelim\PYZus{}2}\PY{p}{]}
\PY{n}{y\PYZus{}OLS\PYZus{}backelim\PYZus{}2\PYZus{}test} \PY{o}{=} \PY{n}{OLS\PYZus{}backelim\PYZus{}2}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}backelim\PYZus{}2\PYZus{}test}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}backelim\PYZus{}2\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}backelim\PYZus{}2\PYZus{}test}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for backwards elimination model using threshold = 0.01: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}backelim\PYZus{}2\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for backwards elimination model using threshold = 0.01: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}backelim\PYZus{}2\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} MSE for forward selection model}
\PY{n}{OLS\PYZus{}forsec\PYZus{}1} \PY{o}{=} \PY{n}{OLS}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}forsec\PYZus{}1\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}OLS\PYZus{}forsec\PYZus{}1\PYZus{}train} \PY{o}{=} \PY{n}{OLS\PYZus{}forsec\PYZus{}1}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}forsec\PYZus{}1\PYZus{}train}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}forsec\PYZus{}1\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}forsec\PYZus{}1\PYZus{}train}\PY{p}{)}

\PY{n}{features\PYZus{}forsec\PYZus{}1} \PY{o}{=} \PY{n}{X\PYZus{}forsec\PYZus{}1\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}

\PY{n}{X\PYZus{}forsec\PYZus{}1\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features\PYZus{}forsec\PYZus{}1}\PY{p}{]}
\PY{n}{y\PYZus{}OLS\PYZus{}forsec\PYZus{}1\PYZus{}test} \PY{o}{=} \PY{n}{OLS\PYZus{}forsec\PYZus{}1}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}forsec\PYZus{}1\PYZus{}test}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}forsec\PYZus{}1\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}forsec\PYZus{}1\PYZus{}test}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for forward selection model using threshold = 0.1: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}forsec\PYZus{}1\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for forward selection model using threshold = 0.1: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}forsec\PYZus{}1\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{n}{OLS\PYZus{}forsec\PYZus{}2} \PY{o}{=} \PY{n}{OLS}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}forsec\PYZus{}2\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}OLS\PYZus{}forsec\PYZus{}2\PYZus{}train} \PY{o}{=} \PY{n}{OLS\PYZus{}forsec\PYZus{}2}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}forsec\PYZus{}2\PYZus{}train}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}forsec\PYZus{}2\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}forsec\PYZus{}2\PYZus{}train}\PY{p}{)}

\PY{n}{features\PYZus{}forsec\PYZus{}2} \PY{o}{=} \PY{n}{X\PYZus{}forsec\PYZus{}2\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}

\PY{n}{X\PYZus{}forsec\PYZus{}2\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{features\PYZus{}forsec\PYZus{}2}\PY{p}{]}
\PY{n}{y\PYZus{}OLS\PYZus{}forsec\PYZus{}2\PYZus{}test} \PY{o}{=} \PY{n}{OLS\PYZus{}forsec\PYZus{}2}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}forsec\PYZus{}2\PYZus{}test}\PY{p}{)}
\PY{n}{MSE\PYZus{}OLS\PYZus{}forsec\PYZus{}2\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}OLS\PYZus{}forsec\PYZus{}2\PYZus{}test}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for forward selection model using threshold = 0.01: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}forsec\PYZus{}2\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for forward selection model using threshold = 0.01: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}OLS\PYZus{}forsec\PYZus{}2\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} MSE for Lasso model }

\PY{c+c1}{\PYZsh{} reg\PYZus{}lasso\PYZus{}cv instance was already defined and fitted earlier, so we can just call on it again}
\PY{n}{y\PYZus{}lasso\PYZus{}cv\PYZus{}train} \PY{o}{=} \PY{n}{reg\PYZus{}lasso\PYZus{}cv}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{MSE\PYZus{}lasso\PYZus{}cv\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}lasso\PYZus{}cv\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}lasso\PYZus{}cv\PYZus{}test} \PY{o}{=} \PY{n}{reg\PYZus{}lasso\PYZus{}cv}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{MSE\PYZus{}lasso\PYZus{}cv\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}lasso\PYZus{}cv\PYZus{}test}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for Lasso model using CV: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}lasso\PYZus{}cv\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for Lasso model using CV: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}lasso\PYZus{}cv\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} reg\PYZus{}lasso\PYZus{}bootstrap instance was also defined and fitted earlier}
\PY{n}{y\PYZus{}lasso\PYZus{}bootstrap\PYZus{}train} \PY{o}{=} \PY{n}{reg\PYZus{}lasso\PYZus{}bootstrap}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)} 
\PY{n}{MSE\PYZus{}lasso\PYZus{}bootstrap\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}lasso\PYZus{}bootstrap\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}lasso\PYZus{}bootstrap\PYZus{}test} \PY{o}{=} \PY{n}{reg\PYZus{}lasso\PYZus{}bootstrap}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{MSE\PYZus{}lasso\PYZus{}bootstrap\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}lasso\PYZus{}bootstrap\PYZus{}test}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for Lasso model using bootstrapping: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}lasso\PYZus{}bootstrap\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for Lasso model using bootstrapping: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}lasso\PYZus{}bootstrap\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}



\PY{c+c1}{\PYZsh{} MSE for GAM model }

\PY{c+c1}{\PYZsh{} gam instance was already defined and fitted earlier}
\PY{n}{y\PYZus{}gam\PYZus{}train} \PY{o}{=} \PY{n}{gam}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{MSE\PYZus{}gam\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}gam\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}gam\PYZus{}test} \PY{o}{=} \PY{n}{gam}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{MSE\PYZus{}gam\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}gam\PYZus{}test}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for GAM model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}gam\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for GAM model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}gam\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} MSE for non\PYZhy{}linear model }

\PY{c+c1}{\PYZsh{} OLS\PYZus{}nonlinear instance was already defined and fitted earlier}

\PY{n}{y\PYZus{}nonlinear\PYZus{}train} \PY{o}{=} \PY{n}{OLS\PYZus{}nonlinear}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}nonlinear}\PY{p}{)}
\PY{n}{MSE\PYZus{}nonlinear\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}nonlinear\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}nonlinear\PYZus{}test} \PY{o}{=} \PY{n}{OLS\PYZus{}nonlinear}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}nonlinear}\PY{p}{)}
\PY{n}{MSE\PYZus{}nonlinear\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}nonlinear\PYZus{}test}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for nonlinear model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}nonlinear\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for nonlinear model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}nonlinear\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} MSE for linear boosting model}

\PY{n}{y\PYZus{}linearboost\PYZus{}train} \PY{o}{=} \PY{o}{\PYZpc{}}\PY{k}{R} linear\PYZus{}boost\PYZdl{}predict(df\PYZus{}train)
\PY{n}{MSE\PYZus{}linearboost\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}linearboost\PYZus{}train}\PY{p}{)}

\PY{n}{y\PYZus{}linearboost\PYZus{}test} \PY{o}{=} \PY{o}{\PYZpc{}}\PY{k}{R} linear\PYZus{}boost\PYZdl{}predict(df\PYZus{}test)
\PY{n}{MSE\PYZus{}linearboost\PYZus{}test} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}linearboost\PYZus{}test}\PY{p}{)}


\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for linear boosting model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}linearboost\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE test for linear boosting model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}linearboost\PYZus{}test:.2f\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} MSE for splines boosting model}

\PY{n}{y\PYZus{}splineboost\PYZus{}train} \PY{o}{=} \PY{o}{\PYZpc{}}\PY{k}{R} spline\PYZus{}boost\PYZdl{}predict(df\PYZus{}train)
\PY{n}{MSE\PYZus{}splineboost\PYZus{}train} \PY{o}{=} \PY{n}{sklm}\PY{o}{.}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}splineboost\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{MSE train for spline boosting model: }\PY{l+s+si}{\PYZob{}MSE\PYZus{}splineboost\PYZus{}train:.2f\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{}y\PYZus{}splineboost\PYZus{}test = \PYZpc{}R spline\PYZus{}boost\PYZdl{}predict(df\PYZus{}test)}
\PY{c+c1}{\PYZsh{}MSE\PYZus{}splineboost\PYZus{}test = sklm.mean\PYZus{}squared\PYZus{}error(y\PYZus{}test, y\PYZus{}splineboost\PYZus{}test)}
\PY{c+c1}{\PYZsh{} Sadly, trying to get test error crashes the program? So I guess I can\PYZsq{}t answer that part}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
MSE test for full OLS model: 0.38
MSE train for full OLS model: 0.31

MSE train for backwards elimination model using threshold = 0.1: 0.34
MSE test for backwards elimination model using threshold = 0.1: 0.37

MSE train for backwards elimination model using threshold = 0.01: 0.37
MSE test for backwards elimination model using threshold = 0.01: 0.40

MSE train for forward selection model using threshold = 0.1: 0.34
MSE test for forward selection model using threshold = 0.1: 0.37

MSE train for forward selection model using threshold = 0.01: 0.34
MSE test for forward selection model using threshold = 0.01: 0.37

MSE train for Lasso model using CV: 0.33
MSE test for Lasso model using CV: 1.50

MSE train for Lasso model using bootstrapping: 0.33
MSE test for Lasso model using bootstrapping: 0.38

MSE train for GAM model: 0.36
MSE test for GAM model: 1.42

MSE train for nonlinear model: 0.29
MSE test for nonlinear model: 0.38

MSE train for linear boosting model: 0.34
MSE test for linear boosting model: 0.38

MSE train for spline boosting model: 0.27
    \end{Verbatim}

    For the boost model errors, I needed to export the models written in R
to Python. This was done using \%R.

The lowest MSE train was gained using boosted splines. Sadly, I can't
seem to get the MSE for the test set and compare. A lot of the models
seem to perform on par with each other. The full OLS model, Lasso model
using bootstrap, boosted linear model, and non-linear model seem to
perform similarly well.

The Lasso model using CV and GAM model seem to have overfitted the data
and give far higher test error than train.

I earlier said I thought the backwards elimination and forward selection
models would perform better than the standard OLS model, but it seems my
threshold was too low and I ended up eliminating too many variables.
Raising the threshold would probably improve these models. The backwards
elimination model using the threshold = 0.1 actually performed better
than the backward elimination with lower threshold and forward
selection, which supports the claim that the models were too strict.

    \hypertarget{problem-2}{%
\section{Problem 2}\label{problem-2}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{R} 
library(mlbench)
data(PimaIndiansDiabetes)
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df\PYZus{}pima} \PY{o}{=} \PY{o}{\PYZpc{}}\PY{k}{R} PimaIndiansDiabetes
\PY{n}{X\PYZus{}pima} \PY{o}{=} \PY{n}{df\PYZus{}pima}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{df\PYZus{}pima}\PY{o}{.}\PY{n}{columns}\PY{o}{!=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{diabetes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{y\PYZus{}pima} \PY{o}{=} \PY{n}{df\PYZus{}pima}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{diabetes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Making pos = True, neg = False, then turning into integers of 1 and 0}
\PY{n}{y\PYZus{}pima} \PY{o}{=} \PY{p}{(}\PY{n}{y\PYZus{}pima}\PY{o}{.}\PY{n}{values} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pos}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{int}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Dividing into training and test sets with same proportion of positive/negative diabetes and training size 2/3}
\PY{n}{Xp\PYZus{}train\PYZus{}}\PY{p}{,} \PY{n}{Xp\PYZus{}test\PYZus{}}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}test} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}pima}\PY{p}{,} \PY{n}{y\PYZus{}pima}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y\PYZus{}pima}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Define the scaler}
\PY{n}{scaler\PYZus{}p} \PY{o}{=} \PY{n}{sklpre}\PY{o}{.}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train\PYZus{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Scale the data with respect to Xp\PYZus{}train }
\PY{n}{Xp\PYZus{}train\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler\PYZus{}p}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Xp\PYZus{}train\PYZus{}}\PY{p}{)}
\PY{n}{Xp\PYZus{}test\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler\PYZus{}p}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Xp\PYZus{}test\PYZus{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Turn back into data frames}
\PY{n}{Xp\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{Xp\PYZus{}train\PYZus{}scaled}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{X\PYZus{}pima}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\PY{n}{Xp\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{Xp\PYZus{}test\PYZus{}scaled}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{X\PYZus{}pima}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{problem-2.1}{%
\subsection{Problem 2.1}\label{problem-2.1}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Creating instance of kNN Classifier}
\PY{n}{kNN\PYZus{}model} \PY{o}{=} \PY{n}{skln}\PY{o}{.}\PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Creating dictionary of k\PYZhy{}values for the grid search}
\PY{n}{k\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{120}\PY{p}{)}
\PY{n}{k\PYZus{}grid} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{k\PYZus{}values}\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} 5\PYZhy{}fold CV grid search}
\PY{n}{kNN\PYZus{}gridsearch\PYZus{}5} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{kNN\PYZus{}model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{k\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}

\PY{c+c1}{\PYZsh{} LOO CV grid search}
\PY{n}{loo} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{LeaveOneOut}\PY{p}{(}\PY{p}{)}
\PY{n}{kNN\PYZus{}gridsearch\PYZus{}loo} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{kNN\PYZus{}model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{k\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{n}{loo}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{41}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Fit the models to the training data with the most optimal k value}
\PY{n}{kNN\PYZus{}train\PYZus{}cv} \PY{o}{=} \PY{n}{kNN\PYZus{}gridsearch\PYZus{}5}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{)}

\PY{n}{kNN\PYZus{}train\PYZus{}loo} \PY{o}{=} \PY{n}{kNN\PYZus{}gridsearch\PYZus{}loo}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Retrieving error rates from the CV results }
\PY{n}{kNN\PYZus{}train\PYZus{}cv\PYZus{}err} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{kNN\PYZus{}train\PYZus{}cv}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{kNN\PYZus{}train\PYZus{}loo\PYZus{}err} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{kNN\PYZus{}train\PYZus{}loo}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}


\PY{c+c1}{\PYZsh{} Calculating test error rates}
\PY{n}{kNN\PYZus{}test\PYZus{}err} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros\PYZus{}like}\PY{p}{(}\PY{n}{k\PYZus{}values}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{float}\PY{p}{)}

\PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n}{k\PYZus{}values}\PY{p}{:}
    \PY{n}{kNN\PYZus{}test} \PY{o}{=} \PY{n}{skln}\PY{o}{.}\PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{k}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{)}
    \PY{n}{kNN\PYZus{}test\PYZus{}err}\PY{p}{[}\PY{n}{k}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{kNN\PYZus{}test}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{Xp\PYZus{}test}\PY{p}{,} \PY{n}{yp\PYZus{}test}\PY{p}{)}
    
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Plotting the error rates over k values}
\PY{n}{figure\PYZus{}kNN} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{k\PYZus{}values}\PY{p}{,} \PY{n}{kNN\PYZus{}train\PYZus{}cv\PYZus{}err}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{5\PYZhy{}fold CV training}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{k\PYZus{}values}\PY{p}{,} \PY{n}{kNN\PYZus{}train\PYZus{}loo\PYZus{}err}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LOO CV training}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{k\PYZus{}values}\PY{p}{,} \PY{n}{kNN\PYZus{}test\PYZus{}err}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test error}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{k}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_63_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    5-fold CV and LOO seem to underestimate the error rate by a lot. The
shape seems to be somewhat similar, which is the most important part, as
we are more interested in finding the value for k that minimizes the
error. These methods are okay for finding the most optimal k, but it is
better to look at test error when you need to consider the actual error
in the model.

However, in this case, it seems that the kNN model is not able to get a
lower error rate than approximately 0.350. The data set itself contains
768 samples, with 268 positive outcomes and we have
\(268/768\approx 0.349\). What probably happens is that kNN starts
predicting only 0's (negative outcomes) when k is sufficiently large,
and because approximately 35\% of the data outcomes are 1's, the error
rate becomes 35\%.

    \hypertarget{problem-2.2}{%
\subsection{Problem 2.2}\label{problem-2.2}}

Considering the bug with the GAM library in Python, I can't do a subset
selection, but I will still fit the model.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{56}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{gam\PYZus{}splines} \PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{s}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Splines for all the features}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{gam\PYZus{}splines} \PY{o}{+}\PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{s}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{spline\PYZus{}order}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{n\PYZus{}splines}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Range of penalties to search}
\PY{n}{penalties} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{150}\PY{p}{)} 
\PY{k}{with} \PY{n}{np}\PY{o}{.}\PY{n}{errstate}\PY{p}{(}\PY{n}{invalid}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{divide}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{over}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Search for optimal penalty }
    \PY{n}{gam\PYZus{}p} \PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{LogisticGAM}\PY{p}{(}\PY{n}{gam\PYZus{}splines}\PY{p}{)}\PY{o}{.}\PY{n}{gridsearch}\PY{p}{(}
        \PY{n}{Xp\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{,} \PY{n}{lam}\PY{o}{=}\PY{n}{penalties}
    \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
100\% (150 of 150) |\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#| Elapsed Time: 0:00:17 Time:  0:00:17
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train error rate using GAM: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}gam\PYZus{}p.accuracy(Xp\PYZus{}train, yp\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test error rate using GAM: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}gam\PYZus{}p.accuracy(Xp\PYZus{}test, yp\PYZus{}test)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Train error rate using GAM: 0.21875
Test error rate using GAM: 0.2421875
    \end{Verbatim}

    GAM seems to perform better than kNN on the training test, but not by a
lot. The test set error rate is noticably higher, indicating perhaps a
slight overfit. Using subset selection would probably improve the model,
but the p-value related bug in pyGAM would have left me with all the
variables regardless as it is prone to underestimating the values.

    \hypertarget{problem-2.3}{%
\subsection{Problem 2.3}\label{problem-2.3}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Trees}
\PY{n}{tree} \PY{o}{=} \PY{n}{skt}\PY{o}{.}\PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the range of tree depth values to grid }
\PY{n}{tree\PYZus{}depths} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{max\PYZus{}depth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Grid search. Uses 5\PYZhy{}fold CV to find optimal tree depth}
\PY{n}{tree\PYZus{}gridsearch} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}
    \PY{n}{tree}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{tree\PYZus{}depths}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best tree depth: }\PY{l+s+si}{\PYZob{}tree\PYZus{}gridsearch.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tree train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}tree\PYZus{}gridsearch.score(Xp\PYZus{}train, yp\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tree test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}tree\PYZus{}gridsearch.score(Xp\PYZus{}test, yp\PYZus{}test)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best tree depth: \{'max\_depth': 4\}
Tree train error rate: 0.185546875
Tree test error rate: 0.296875
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{47}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Bagging}
\PY{n}{bagging} \PY{o}{=} \PY{n}{skle}\PY{o}{.}\PY{n}{BaggingClassifier}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define the range of number of trees to grid}
\PY{n}{n\PYZus{}trees} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Grid search. Uses 5\PYZhy{}fold CV to find optimal hyperparameter}
\PY{n}{bagging\PYZus{}gridsearch} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}
    \PY{n}{bagging}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{n\PYZus{}trees}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best number of base estimators (bagging): }\PY{l+s+si}{\PYZob{}bagging\PYZus{}gridsearch.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bagging train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}bagging\PYZus{}gridsearch.score(Xp\PYZus{}train, yp\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bagging test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}bagging\PYZus{}gridsearch.score(Xp\PYZus{}test, yp\PYZus{}test)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best number of base estimators (bagging): \{'n\_estimators': 61\}
Bagging train error rate: 0.0
Bagging test error rate: 0.25
    \end{Verbatim}

    Note: This is only for probability. Scikit-learn does not have an
implementation for consensus votes as it tends to perform worse than
probability.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Random forest }
\PY{n}{randomforest} \PY{o}{=} \PY{n}{skle}\PY{o}{.}\PY{n}{RandomForestClassifier}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define range of number of estimators and max tree depths to search }
\PY{n}{hyperparams\PYZus{}forest} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{max\PYZus{}depth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{30}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Randomized search. Uses 5\PYZhy{}fold CV to find optimal hyperparameters}
\PY{n}{randomforest\PYZus{}search} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{RandomizedSearchCV}\PY{p}{(}
    \PY{n}{randomforest}\PY{p}{,} \PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{n}{hyperparams\PYZus{}forest}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best number of base estimators and max depth (random forest): }\PY{l+s+si}{\PYZob{}randomforest\PYZus{}search.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Random forest train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} randomforest\PYZus{}search.score(Xp\PYZus{}train, yp\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Random forest test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} randomforest\PYZus{}search.score(Xp\PYZus{}test, yp\PYZus{}test)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best number of base estimators and max depth (random forest): \{'n\_estimators':
76, 'max\_depth': 26\}
Random forest train error rate: 0.0
Random forest test error rate: 0.25
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Neural network }
\PY{n}{NN} \PY{o}{=} \PY{n}{sknn}\PY{o}{.}\PY{n}{MLPClassifier}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}sizes}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define range of learning rate, shrinkage and batch size to search}
\PY{n}{hyperparams\PYZus{}nn} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{learning\PYZus{}rate\PYZus{}init}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,} 
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{alpha}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,} 
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{batch\PYZus{}size}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}  \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{400}\PY{p}{)}
\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Randomized search. Uses 5\PYZhy{}fold CV to find optimal hyperparameters}
\PY{n}{NN\PYZus{}search} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{RandomizedSearchCV}\PY{p}{(}
    \PY{n}{NN}\PY{p}{,} \PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{n}{hyperparams\PYZus{}nn}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{150}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best learning rate, shrinkage alpha and batch size (NN): }\PY{l+s+si}{\PYZob{}NN\PYZus{}search.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Neural network train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} NN\PYZus{}search.score(Xp\PYZus{}train, yp\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Neural network test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} NN\PYZus{}search.score(Xp\PYZus{}test, yp\PYZus{}test)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best learning rate, shrinkage alpha and batch size (NN): \{'learning\_rate\_init':
0.00015922827933410923, 'batch\_size': 257, 'alpha': 0.008697490026177835\}
Neural network train error rate: 0.197265625
Neural network test error rate: 0.25
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/lise/.local/share/virtualenvs/STK-
IN4300-assignment2-hpF4PA7F/lib/python3.6/site-
packages/sklearn/neural\_network/multilayer\_perceptron.py:566:
ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and
the optimization hasn't converged yet.
  \% self.max\_iter, ConvergenceWarning)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{50}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{adaboost} \PY{o}{=} \PY{n}{skle}\PY{o}{.}\PY{n}{AdaBoostClassifier}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Define range of number of estimators and learning rate to search}
\PY{n}{hyperparams\PYZus{}ada} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{300}\PY{p}{)}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{learning\PYZus{}rate}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{\PYZcb{}}

\PY{c+c1}{\PYZsh{} Randomized search. Uses 5\PYZhy{}fold CV to find optimal hyperparameters}
\PY{n}{adaboost\PYZus{}search} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{RandomizedSearchCV}\PY{p}{(}
    \PY{n}{adaboost}\PY{p}{,} \PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{n}{hyperparams\PYZus{}ada}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{150}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp\PYZus{}train}\PY{p}{,} \PY{n}{yp\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best number of estimators and learning rate (AdaBoost): }\PY{l+s+si}{\PYZob{}adaboost\PYZus{}search.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AdaBoost train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} adaboost\PYZus{}search.score(Xp\PYZus{}train, yp\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AdaBoost test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} adaboost\PYZus{}search.score(Xp\PYZus{}test, yp\PYZus{}test)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Best number of estimators and learning rate (AdaBoost): \{'n\_estimators': 190,
'learning\_rate': 0.026560877829466867\}
AdaBoost train error rate: 0.21484375
AdaBoost test error rate: 0.25
    \end{Verbatim}

    \hypertarget{problem-2.4}{%
\subsection{Problem 2.4}\label{problem-2.4}}

I would probably use AdaBoost, as the tree methods (Decision trees,
bagging, random forests) seem to easily overfit and the neural network
requires more hyperparameter tuning. AdaBoost and Neural network seem to
perform practically identically in this case, but AdaBoost actually
requires one fewer hyperparameter to tune, which makes it a lot more
practical to use. kNN was basically useless.

    \hypertarget{problem-2.5}{%
\subsection{Problem 2.5}\label{problem-2.5}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{51}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{R} 
library(mlbench)
data(PimaIndiansDiabetes2)
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{52}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{df\PYZus{}pima2} \PY{o}{=} \PY{o}{\PYZpc{}}\PY{k}{R} PimaIndiansDiabetes2

\PY{c+c1}{\PYZsh{} Dropping all NA values}
\PY{n}{df\PYZus{}pima2}\PY{o}{.}\PY{n}{dropna}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Then basically same process as we did for the others}
\PY{n}{X\PYZus{}pima2} \PY{o}{=} \PY{n}{df\PYZus{}pima2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{df\PYZus{}pima2}\PY{o}{.}\PY{n}{columns}\PY{o}{!=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{diabetes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
\PY{n}{y\PYZus{}pima2} \PY{o}{=} \PY{n}{df\PYZus{}pima2}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{diabetes}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}

\PY{n}{y\PYZus{}pima2} \PY{o}{=} \PY{p}{(}\PY{n}{y\PYZus{}pima2}\PY{o}{.}\PY{n}{values} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{pos}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}


\PY{n}{Xp2\PYZus{}train\PYZus{}}\PY{p}{,} \PY{n}{Xp2\PYZus{}test\PYZus{}}\PY{p}{,} \PY{n}{yp2\PYZus{}train}\PY{p}{,} \PY{n}{yp2\PYZus{}test} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}pima2}\PY{p}{,} \PY{n}{y\PYZus{}pima2}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{y\PYZus{}pima2}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{/}\PY{l+m+mi}{3}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Define the scaler}
\PY{n}{scaler\PYZus{}p2} \PY{o}{=} \PY{n}{sklpre}\PY{o}{.}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp2\PYZus{}train\PYZus{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Scale the data with respect to Xp2\PYZus{}train }
\PY{n}{Xp2\PYZus{}train\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler\PYZus{}p2}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Xp2\PYZus{}train\PYZus{}}\PY{p}{)}
\PY{n}{Xp2\PYZus{}test\PYZus{}scaled} \PY{o}{=} \PY{n}{scaler\PYZus{}p2}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{Xp2\PYZus{}test\PYZus{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Turn back into data frames}
\PY{n}{Xp2\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{Xp2\PYZus{}train\PYZus{}scaled}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{X\PYZus{}pima2}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\PY{n}{Xp2\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{Xp2\PYZus{}test\PYZus{}scaled}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{X\PYZus{}pima2}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Now to repeat all the methods:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{58}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} GAM }
\PY{n}{gam\PYZus{}splines2} \PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{s}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{Xp2\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{gam\PYZus{}splines2} \PY{o}{+}\PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{s}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{spline\PYZus{}order}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{n\PYZus{}splines}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}

\PY{k}{with} \PY{n}{np}\PY{o}{.}\PY{n}{errstate}\PY{p}{(}\PY{n}{invalid}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{divide}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{over}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ignore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
    \PY{n}{gam\PYZus{}p2} \PY{o}{=} \PY{n}{pygam}\PY{o}{.}\PY{n}{LogisticGAM}\PY{p}{(}\PY{n}{gam\PYZus{}splines2}\PY{p}{)}\PY{o}{.}\PY{n}{gridsearch}\PY{p}{(}
        \PY{n}{Xp2\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{p}{,} \PY{n}{yp2\PYZus{}train}\PY{p}{,} \PY{n}{lam}\PY{o}{=}\PY{n}{penalties}
    \PY{p}{)}
    
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Train error rate using GAM: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}gam\PYZus{}p2.accuracy(Xp2\PYZus{}train, yp2\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Test error rate using GAM: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}gam\PYZus{}p2.accuracy(Xp2\PYZus{}test, yp2\PYZus{}test)\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} kNN}
\PY{n}{kNN2\PYZus{}gridsearch\PYZus{}5} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{kNN\PYZus{}model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{k\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp2\PYZus{}train}\PY{p}{,} \PY{n}{yp2\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best k (kNN): }\PY{l+s+si}{\PYZob{}kNN2\PYZus{}gridsearch\PYZus{}5.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kNN train error rate (kNN): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}kNN2\PYZus{}gridsearch\PYZus{}5.score(Xp2\PYZus{}train, yp2\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{kNN test error rate (kNN): }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}kNN2\PYZus{}gridsearch\PYZus{}5.score(Xp2\PYZus{}test, yp2\PYZus{}test)\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Trees}
\PY{n}{tree\PYZus{}gridsearch2} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}
    \PY{n}{tree}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{tree\PYZus{}depths}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp2\PYZus{}train}\PY{p}{,} \PY{n}{yp2\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best tree depth: }\PY{l+s+si}{\PYZob{}tree\PYZus{}gridsearch2.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tree train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}tree\PYZus{}gridsearch2.score(Xp2\PYZus{}train, yp2\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tree test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}tree\PYZus{}gridsearch2.score(Xp2\PYZus{}test, yp2\PYZus{}test)\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Bagging}
\PY{n}{bagging\PYZus{}gridsearch2} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{GridSearchCV}\PY{p}{(}
    \PY{n}{bagging}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{n\PYZus{}trees}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp2\PYZus{}train}\PY{p}{,} \PY{n}{yp2\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best number of base estimators (bagging): }\PY{l+s+si}{\PYZob{}bagging\PYZus{}gridsearch2.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bagging train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}bagging\PYZus{}gridsearch2.score(Xp2\PYZus{}train, yp2\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Bagging test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1\PYZhy{}bagging\PYZus{}gridsearch2.score(Xp2\PYZus{}test, yp2\PYZus{}test)\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Random forest }
\PY{n}{randomforest\PYZus{}search2} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{RandomizedSearchCV}\PY{p}{(}
    \PY{n}{randomforest}\PY{p}{,} \PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{n}{hyperparams\PYZus{}forest}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp2\PYZus{}train}\PY{p}{,} \PY{n}{yp2\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best number of base estimators and max depth (random forest): }\PY{l+s+si}{\PYZob{}randomforest\PYZus{}search2.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Random forest train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} randomforest\PYZus{}search2.score(Xp2\PYZus{}train, yp2\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Random forest test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} randomforest\PYZus{}search2.score(Xp2\PYZus{}test, yp2\PYZus{}test)\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Neural network}
\PY{n}{NN\PYZus{}search2} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{RandomizedSearchCV}\PY{p}{(}
    \PY{n}{NN}\PY{p}{,} \PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{n}{hyperparams\PYZus{}nn}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{150}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp2\PYZus{}train}\PY{p}{,} \PY{n}{yp2\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best learning rate, shrinkage alpha and batch size (NN): }\PY{l+s+si}{\PYZob{}NN\PYZus{}search2.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Neural network train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} NN\PYZus{}search2.score(Xp2\PYZus{}train, yp2\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Neural network test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} NN\PYZus{}search2.score(Xp2\PYZus{}test, yp2\PYZus{}test)\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Adaptive boosting}
\PY{n}{adaboost\PYZus{}search2} \PY{o}{=} \PY{n}{sklms}\PY{o}{.}\PY{n}{RandomizedSearchCV}\PY{p}{(}
    \PY{n}{adaboost}\PY{p}{,} \PY{n}{param\PYZus{}distributions}\PY{o}{=}\PY{n}{hyperparams\PYZus{}ada}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{150}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{iid}\PY{o}{=}\PY{k+kc}{False}
\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Xp2\PYZus{}train}\PY{p}{,} \PY{n}{yp2\PYZus{}train}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Best number of estimators and learning rate (AdaBoost): }\PY{l+s+si}{\PYZob{}adaboost\PYZus{}search2.best\PYZus{}params\PYZus{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AdaBoost train error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} adaboost\PYZus{}search2.score(Xp2\PYZus{}train, yp2\PYZus{}train)\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AdaBoost test error rate: }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{1 \PYZhy{} adaboost\PYZus{}search2.score(Xp2\PYZus{}test, yp2\PYZus{}test)\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
100\% (150 of 150) |\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#| Elapsed Time: 0:00:16 Time:  0:00:16
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Train error rate using GAM: 0.1954022988505747
Test error rate using GAM: 0.22137404580152675

Best k (kNN): \{'n\_neighbors': 9\}
kNN train error rate (kNN): 0.2030651340996169
kNN test error rate (kNN): 0.26717557251908397

Best tree depth: \{'max\_depth': 14\}
Tree train error rate: 0.0
Tree test error rate: 0.29007633587786263

Best number of base estimators (bagging): \{'n\_estimators': 62\}
Bagging train error rate: 0.0
Bagging test error rate: 0.2137404580152672

Best number of base estimators and max depth (random forest): \{'n\_estimators':
63, 'max\_depth': 19\}
Random forest train error rate: 0.0
Random forest test error rate: 0.19083969465648853

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/lise/.local/share/virtualenvs/STK-
IN4300-assignment2-hpF4PA7F/lib/python3.6/site-
packages/sklearn/neural\_network/multilayer\_perceptron.py:350: UserWarning: Got
`batch\_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn("Got `batch\_size` less than 1 or larger than "
/home/lise/.local/share/virtualenvs/STK-
IN4300-assignment2-hpF4PA7F/lib/python3.6/site-
packages/sklearn/neural\_network/multilayer\_perceptron.py:566:
ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and
the optimization hasn't converged yet.
  \% self.max\_iter, ConvergenceWarning)
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Best learning rate, shrinkage alpha and batch size (NN): \{'learning\_rate\_init':
0.0013530477745798076, 'batch\_size': 386, 'alpha': 0.00025353644939701115\}
Neural network train error rate: 0.01532567049808431
Neural network test error rate: 0.25190839694656486

Best number of estimators and learning rate (AdaBoost): \{'n\_estimators': 211,
'learning\_rate': 0.026560877829466867\}
AdaBoost train error rate: 0.16475095785440608
AdaBoost test error rate: 0.2137404580152672

    \end{Verbatim}

    The tree methods (single tree, bagging, random forest) are all still
overfitting by a lot, with training errors all reaching 0. Of the three,
the single tree is the worst model and random forest is the best.

GAM and kNN are not able to reach as low training errors as the tree
methods, but they are at least not overfitting to the same extent as the
tree methods. However, their test error rates are still higher than that
of the random forest model. I would say GAM actually performs better
than I expected with a test error rate of 0.22. The kNN and single tree
perform the worst overall, with the single tree having the largest test
error rate at 0.29.

For the chosen hyperparameters, both the neural network and AdaBoost
models seem to be overfitting the data, albeit the AdaBoost to a far
lesser extent than the neural network. The neural network actually
performs worse than bagging and random forests due to the way neural
networks overfit the data. Overall, the random forest has the lowest
test error rate, while AdaBoost and bagging seem to have the same test
error rate, which is quite interesting. In this particular case I guess
I would have chosen the random forest model.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
